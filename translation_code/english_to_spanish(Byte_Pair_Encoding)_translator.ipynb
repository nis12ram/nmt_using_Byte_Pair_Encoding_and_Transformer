{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qe4n0y_He3Km"
      },
      "outputs": [],
      "source": [
        "from byte_pair_level_transformer import Transformer\n",
        "import torch\n",
        "import numpy as np\n",
        "import re\n",
        "import collections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1pBj3ma7hVc4"
      },
      "outputs": [],
      "source": [
        "file_path = '/content/drive/MyDrive/spa.txt'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ge2rvFA0yZuR"
      },
      "outputs": [],
      "source": [
        "def text_preprocessing(text):\n",
        "  # text = re.sub(r\"([?.!,¿])\", r\" \\1 \", text)\n",
        "  text = re.sub(r'[\" \"]+', \" \", text)\n",
        "  # text = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", text)\n",
        "  text = re.sub(r\"[^a-zA-Z]+\", \" \", text)\n",
        "\n",
        "  text = text.strip().lower()\n",
        "\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ViD2B7hgyuKx"
      },
      "outputs": [],
      "source": [
        "num_data = 70000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8zWU4rSJik85"
      },
      "outputs": [],
      "source": [
        "with open(file_path,'r') as f:\n",
        "  lines = f.readlines()\n",
        "english_sentence,spanish_sentence = [],[]\n",
        "for total_example,line in enumerate(lines):\n",
        "  if (total_example < num_data ):\n",
        "    line = line.lower()\n",
        "    data = line.replace(\"\\n\",\"\").split(\"\\t\")\n",
        "    data[0] = text_preprocessing(data[0])\n",
        "    data[1] = text_preprocessing(data[1])\n",
        "    english_sentence.append(data[0])\n",
        "    spanish_sentence.append(data[1])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "S6x1jFuOR7tn"
      },
      "outputs": [],
      "source": [
        "spanish_vocabulary = []\n",
        "english_vocabulary = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "t_3PTj5uLsiw"
      },
      "outputs": [],
      "source": [
        "for en,sp in zip(english_sentence,spanish_sentence):\n",
        "  en_tokens = en.split()\n",
        "  sp_tokens = sp.split()\n",
        "  for en_token,sp_token in zip(en_tokens,sp_tokens):\n",
        "    if en_token not in english_vocabulary:\n",
        "      english_vocabulary.append(en_token)\n",
        "    if sp_token not in spanish_vocabulary:\n",
        "      spanish_vocabulary.append(sp_token)\n",
        "  # print(english_vocabulary)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8Dv2I1FOKub",
        "outputId": "ca795012-744d-4673-ba8a-290af9ecc594"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(7985, 14068)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(english_vocabulary),len(spanish_vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "a8BnIXcSngHJ"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "en_word_freq_dict  = collections.defaultdict(int)\n",
        "\n",
        "for word in english_vocabulary:\n",
        "  en_word_freq_dict[\" \".join(word) + ' #'] += 1\n",
        "# en_word_freq_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qr0fiCGbn7OQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "sp_word_freq_dict  = collections.defaultdict(int)\n",
        "\n",
        "for word in spanish_vocabulary:\n",
        "  sp_word_freq_dict[\" \".join(word) + ' #'] += 1\n",
        "# sp_word_freq_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "t8xezAN3nDI-"
      },
      "outputs": [],
      "source": [
        "def get_pairs(word_freq_dict):\n",
        "  '''\n",
        "  goal:\n",
        "      used to get the pairs dict:\n",
        "                key(tuple{str,str}): represent the byte pairs\n",
        "                values(int): represent the frequency of the byte pair\n",
        "      and return the pairs dict\n",
        "  '''\n",
        "  pairs = collections.defaultdict(int)\n",
        "  for word, freq in word_freq_dict.items():\n",
        "    chars = word.split()\n",
        "    for i in range(len(chars)-1):\n",
        "      pairs[chars[i],chars[i+1]] += freq\n",
        "  return pairs\n",
        "\n",
        "def merge_byte_pairs(best_pair, word_freq_dict):\n",
        "  '''\n",
        "  goal:\n",
        "      used to merge the byte pairs that has highest frequency\n",
        "      and return the merged dict{new word_freq_dict}\n",
        "  '''\n",
        "  # print(best_pair)\n",
        "  merged_dict = {}\n",
        "  bigram = re.escape(' '.join(best_pair))\n",
        "  # print(f'bigram {bigram}')\n",
        "  p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
        "  # print(f'p {p}')\n",
        "  for word in word_freq_dict:\n",
        "    # print(word)\n",
        "    w_out = p.sub(''.join(best_pair), word) # merging best byte pair.\n",
        "    # print(f'w_out {w_out}')\n",
        "    merged_dict[w_out] = word_freq_dict[word]\n",
        "  return merged_dict\n",
        "\n",
        "def get_subword_tokens(word_freq_dict):\n",
        "  char_freq_dict = collections.defaultdict(int)\n",
        "  for word,freq in word_freq_dict.items():\n",
        "    chars = word.split()\n",
        "    for char in chars:\n",
        "      char_freq_dict[char] += freq\n",
        "  return char_freq_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "iZa_uxwdojLB"
      },
      "outputs": [],
      "source": [
        "for i in range(9000):\n",
        "  # if (i == 0):\n",
        "    # print(f'{get_subword_tokens(en_word_freq_dict)}')\n",
        "    # print('')\n",
        "  pairs = get_pairs(en_word_freq_dict)\n",
        "  try:\n",
        "    best_pair = max(pairs,key = pairs.get)\n",
        "  except:\n",
        "    break\n",
        "  # print(f\"Iteration {i}: \")\n",
        "  en_word_freq_dict = merge_byte_pairs(best_pair,en_word_freq_dict)\n",
        "  en_subword_tokens = get_subword_tokens(en_word_freq_dict)\n",
        "  # print(en_subword_tokens)\n",
        "  # print(len(en_subword_tokens))\n",
        "  # print()\n",
        "  i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Zf1X-CK3o2iY"
      },
      "outputs": [],
      "source": [
        "\n",
        "for i in range(9000):\n",
        "  # if (i == 0):\n",
        "  #   print(f'{get_subword_tokens(sp_word_freq_dict)}')\n",
        "  #   print('')\n",
        "  pairs = get_pairs(sp_word_freq_dict)\n",
        "  try:\n",
        "    best_pair = max(pairs,key = pairs.get)\n",
        "  except:\n",
        "    break\n",
        "  # print(f\"Iteration {i}: \")\n",
        "  sp_word_freq_dict = merge_byte_pairs(best_pair,sp_word_freq_dict)\n",
        "  sp_subword_tokens = get_subword_tokens(sp_word_freq_dict)\n",
        "  # print(sp_subword_tokens)\n",
        "  # print(len(sp_subword_tokens))\n",
        "  # print()\n",
        "  i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "pnqg1ws4dQsK"
      },
      "outputs": [],
      "source": [
        "def measure_token_length(token):\n",
        "    if token[-4:] == '#':\n",
        "        return len(token[:-4]) + 1\n",
        "    else:\n",
        "        return len(token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPn88g2Nkhg6"
      },
      "source": [
        "Byte pair vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "elC0oO-qdGSq"
      },
      "outputs": [],
      "source": [
        "en_sorted_tokens_tuple = sorted(en_subword_tokens.items(), key=lambda item: (measure_token_length(item[0]), item[1]), reverse=True)\n",
        "sp_sorted_tokens_tuple = sorted(sp_subword_tokens.items(), key=lambda item: (measure_token_length(item[0]), item[1]), reverse=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-XffxrmeeHr",
        "outputId": "5a2b64fd-703e-486c-c8f7-6d393614d62a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(('hyperventilating#', 1), ('q', 1))"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "en_sorted_tokens_tuple[0],en_sorted_tokens_tuple[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "0bp_g3MF5iL2"
      },
      "outputs": [],
      "source": [
        "en_vocab_tokenization = [token for (token, freq) in en_sorted_tokens_tuple]\n",
        "sp_vocab_tokenization = [token for (token, freq) in sp_sorted_tokens_tuple]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73vakBe85kjb",
        "outputId": "df2f7cab-cd5a-4d34-e9c1-32949e8f986d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(7322, 8354)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(en_vocab_tokenization),len(sp_vocab_tokenization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJKXNramfnv3",
        "outputId": "4c08df31-1422-4e57-94ba-9de20d717f9f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['hyperventilating#',\n",
              " 'congratulations#',\n",
              " 'understandable#',\n",
              " 'claustrophobic#',\n",
              " 'underestimated#',\n",
              " 'disappointment#',\n",
              " 'discrimination#',\n",
              " 'concentrating#',\n",
              " 'misunderstood#',\n",
              " 'flabbergasted#']"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "en_vocab_tokenization[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "1irLJNPW5m7a"
      },
      "outputs": [],
      "source": [
        "en_lng_to_index = {}\n",
        "en_index_to_lng = {}\n",
        "k = 4\n",
        "for i in en_vocab_tokenization:\n",
        "  en_lng_to_index[i] = k\n",
        "  en_index_to_lng[k] = i\n",
        "  k += 1\n",
        "\n",
        "\n",
        "sp_lng_to_index = {}\n",
        "sp_index_to_lng = {}\n",
        "k = 4\n",
        "for i in sp_vocab_tokenization:\n",
        "  sp_lng_to_index[i] = k\n",
        "  sp_index_to_lng[k] = i\n",
        "  k += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "-T2GFwgc2cuV"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "START_TOKEN = '<START>'\n",
        "PADDING_TOKEN = '<PAD>'\n",
        "END_TOKEN = '<END>'\n",
        "\n",
        "en_index_to_lng[0] = '<OOV>'\n",
        "sp_index_to_lng[0] = '<OOV>'\n",
        "en_index_to_lng[1] = '<START>'\n",
        "sp_index_to_lng[1] = '<START>'\n",
        "en_index_to_lng[2] = '<END>'\n",
        "sp_index_to_lng[2] = '<END>'\n",
        "en_index_to_lng[3] = '<PAD>'\n",
        "sp_index_to_lng[3] = '<PAD>'\n",
        "\n",
        "en_lng_to_index['<OOV>'] = 0\n",
        "sp_lng_to_index['<OOV>'] = 0\n",
        "en_lng_to_index['<START>'] = 1\n",
        "sp_lng_to_index['<START>'] = 1\n",
        "en_lng_to_index['<END>'] = 2\n",
        "sp_lng_to_index['<END>'] = 2\n",
        "en_lng_to_index['<PAD>'] = 3\n",
        "sp_lng_to_index['<PAD>'] = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhO0WBLX5pXF",
        "outputId": "b2f16798-1dc8-4e97-eacd-a07d36278818"
      },
      "outputs": [],
      "source": [
        "en_lng_to_index,sp_lng_to_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hNF0kdQ5zGW",
        "outputId": "95fe01d0-f98a-4156-bc66-512f9c02df2e"
      },
      "outputs": [],
      "source": [
        "len(english_sentence),len(spanish_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0gpYOyFkQFy",
        "outputId": "c6c78dd1-5336-45c7-a227-b89db042edb6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['go', 'go', 'go', 'go', 'hi', 'run', 'run', 'who', 'fire', 'fire']"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "english_sentence[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dv7VWb8GkWHh",
        "outputId": "ba8f6953-3a2a-46f6-89e8-9a342ff8b724"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['ve',\n",
              " 'vete',\n",
              " 'vaya',\n",
              " 'v yase',\n",
              " 'hola',\n",
              " 'corre',\n",
              " 'corred',\n",
              " 'qui n',\n",
              " 'fuego',\n",
              " 'incendio']"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spanish_sentence[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYljiEZkkiA0",
        "outputId": "9abc2458-63fc-45db-d38f-8a3c88629128"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(19, 66)"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max(len(x) for x in english_sentence),max(len(x) for x in spanish_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1g1mTQwkwt6",
        "outputId": "1de9af76-0405-4d82-bba0-2258e1a2c9dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "14.9992\n",
            "16.39655\n"
          ]
        }
      ],
      "source": [
        "# computing avg length\n",
        "print(sum(len(x) for x in english_sentence)/len(english_sentence))\n",
        "print(sum(len(x) for x in spanish_sentence)/len(spanish_sentence))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "vtca6awt9J_z"
      },
      "outputs": [],
      "source": [
        "d_model = 152\n",
        "batch_size = 16\n",
        "ffn_hidden = 152\n",
        "num_heads = 2\n",
        "drop_prob = 0.1\n",
        "num_stacked = 1\n",
        "max_token_length = 7\n",
        "sp_vocab_size = len(sp_lng_to_index)\n",
        "en_vocab_size = len(en_lng_to_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkbD6SsvsbYv"
      },
      "source": [
        "limit the length of the sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhCxoQqzq4o3",
        "outputId": "bd7a5a0f-f670-46f3-8f76-42d40853c56b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(19, 66)"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max(len(x) for x in english_sentence),max(len(x) for x in spanish_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "O5am0HOPuWHz"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "  '''\n",
        "  overriding certain methods of the Dataset class\n",
        "  '''\n",
        "  def __init__(self,english_sentence,spanish_sentence):\n",
        "    self.english_sentence = english_sentence\n",
        "    self.spanish_sentence = spanish_sentence\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.english_sentence)\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    return self.english_sentence[index],self.spanish_sentence[index]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "Y0q7IGjZvVEH"
      },
      "outputs": [],
      "source": [
        "dataset = TextDataset(english_sentence,spanish_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eI5iOYbvX77",
        "outputId": "9aa06219-384e-4fc6-9b27-2456cf982bad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "20000"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "KRLIKa_tv_CJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_loader = DataLoader(dataset,batch_size=batch_size)\n",
        "iterator = iter(train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBTgtR9cwUSA",
        "outputId": "e4d9af25-f902-4b07-cbdb-676766e1a554"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('go', 'go', 'go', 'go', 'hi', 'run', 'run', 'who', 'fire', 'fire', 'fire', 'help', 'help', 'help', 'jump', 'jump'), ('ve', 'vete', 'vaya', 'v yase', 'hola', 'corre', 'corred', 'qui n', 'fuego', 'incendio', 'disparad', 'ayuda', 'socorro auxilio', 'auxilio', 'salta', 'salte')]\n",
            "[('stop', 'stop', 'stop', 'wait', 'wait', 'go on', 'go on', 'hello', 'i ran', 'i ran', 'i try', 'i won', 'oh no', 'relax', 'smile', 'attack'), ('parad', 'para', 'pare', 'espera', 'esperen', 'contin a', 'contin e', 'hola', 'corr', 'corr a', 'lo intento', 'he ganado', 'oh no', 'tom telo con soda', 'sonr e', 'al ataque')]\n",
            "[('attack', 'get up', 'go now', 'got it', 'got it', 'got it', 'he ran', 'hop in', 'hug me', 'i fell', 'i know', 'i left', 'i lied', 'i lost', 'i quit', 'i quit'), ('atacad', 'levanta', 've ahora mismo', 'lo tengo', 'lo pillas', 'entendiste', 'l corri', 'm tete adentro', 'abr zame', 'me ca', 'yo lo s', 'sal', 'ment', 'perd', 'dimito', 'renunci')]\n",
            "[('i work', 'i m', 'i m up', 'listen', 'listen', 'listen', 'no way', 'no way', 'no way', 'no way', 'no way', 'no way', 'no way', 'no way', 'no way', 'no way'), ('estoy trabajando', 'tengo diecinueve', 'estoy levantado', 'escucha', 'escuche', 'escuchen', 'no puede ser', 'de ninguna manera', 'de ninguna manera', 'imposible', 'de ning n modo', 'de eso nada', 'ni cagando', 'mangos', 'minga', 'ni en pedo')]\n",
            "[('really', 'really', 'thanks', 'thanks', 'try it', 'we try', 'we won', 'why me', 'ask tom', 'awesome', 'be calm', 'be cool', 'be fair', 'be kind', 'be nice', 'beat it'), ('en serio', 'la verdad', 'gracias', 'gracias', 'pru balo', 'lo procuramos', 'ganamos', 'por qu yo', 'preg ntale a tom', 'rale', 'mantente en calma', 'estate tranquilo', 's justo', 'sean gentiles', 's agradable', 'p rate')]\n"
          ]
        }
      ],
      "source": [
        "for batch_num,batch in enumerate(iterator):\n",
        "  print(batch)\n",
        "  # break\n",
        "  if (batch_num > 3):\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "eVHzNQWAA8g2"
      },
      "outputs": [],
      "source": [
        "transformer = Transformer(d_model,\n",
        "                          ffn_hidden,\n",
        "                          num_heads,\n",
        "                          drop_prob,\n",
        "                          num_stacked,\n",
        "                          max_token_length,\n",
        "                          sp_vocab_size,\n",
        "                          en_lng_to_index,\n",
        "                          sp_lng_to_index,\n",
        "                          START_TOKEN,\n",
        "                          END_TOKEN,\n",
        "                          PADDING_TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkQmg8k5DCXh",
        "outputId": "14ed3785-470a-45c6-f92d-9b44fe2bf251"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (encoder): Encoder(\n",
              "    (sentence_embedding): SentenceEmbedding(\n",
              "      (embedding): Embedding(7326, 152)\n",
              "      (position_encoder): AbsolutePositionalEncoding()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (layers): SequentialEncoder(\n",
              "      (0): EncoderLayer(\n",
              "        (attention): MultiHeadAttention(\n",
              "          (qkv_layer): Linear(in_features=152, out_features=456, bias=True)\n",
              "          (linear_layer): Linear(in_features=152, out_features=152, bias=True)\n",
              "        )\n",
              "        (norm1): LayerNormalization()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (ffn): PositionalwiseFeedForwrd(\n",
              "          (linear1): Linear(in_features=152, out_features=152, bias=True)\n",
              "          (linear2): Linear(in_features=152, out_features=152, bias=True)\n",
              "          (relu): ReLU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (norm2): LayerNormalization()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (sentence_embedding): SentenceEmbedding(\n",
              "      (embedding): Embedding(8358, 152)\n",
              "      (position_encoder): AbsolutePositionalEncoding()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (layers): SequentialDecoder(\n",
              "      (0): DecoderLayer(\n",
              "        (masked_attention): MultiHeadAttention(\n",
              "          (qkv_layer): Linear(in_features=152, out_features=456, bias=True)\n",
              "          (linear_layer): Linear(in_features=152, out_features=152, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (norm1): LayerNormalization()\n",
              "        (encoder_decoder_attention): MultiHeadCrossAttention(\n",
              "          (kv_layer): Linear(in_features=152, out_features=304, bias=True)\n",
              "          (q_layer): Linear(in_features=152, out_features=152, bias=True)\n",
              "          (linear_layer): Linear(in_features=152, out_features=152, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNormalization()\n",
              "        (ffn): PositionalwiseFeedForwrd(\n",
              "          (linear1): Linear(in_features=152, out_features=152, bias=True)\n",
              "          (linear2): Linear(in_features=152, out_features=152, bias=True)\n",
              "          (relu): ReLU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (norm3): LayerNormalization()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (linear): Linear(in_features=152, out_features=8358, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "meuR2adX-h8y"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "criterian = nn.CrossEntropyLoss(ignore_index=sp_lng_to_index[PADDING_TOKEN],\n",
        "                                reduction='none')\n",
        "\n",
        "# When computing the loss, we are ignoring cases when the label is the padding token\n",
        "for params in transformer.parameters():\n",
        "    if params.dim() > 1:\n",
        "        nn.init.xavier_uniform_(params)\n",
        "\n",
        "optim = torch.optim.Adam(transformer.parameters(), lr=1e-4)\n",
        "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "GTV2rZfRDk15"
      },
      "outputs": [],
      "source": [
        "NEG_INFTY = -1e9\n",
        "\n",
        "def create_masks(eng_batch, sp_batch):\n",
        "    num_sentences = len(eng_batch) # {represent batch size}\n",
        "    look_ahead_mask = torch.full([max_token_length, max_token_length] , True)\n",
        "    look_ahead_mask = torch.triu(look_ahead_mask, diagonal=1)\n",
        "    encoder_padding_mask = torch.full([num_sentences, max_token_length, max_token_length] , False)\n",
        "    decoder_padding_mask_self_attention = torch.full([num_sentences, max_token_length, max_token_length] , False)\n",
        "    decoder_padding_mask_cross_attention = torch.full([num_sentences, max_token_length, max_token_length] , False)\n",
        "\n",
        "    for idx in range(num_sentences):\n",
        "      eng_sentence_length, sp_sentence_length = len(eng_batch[idx]), len(sp_batch[idx])\n",
        "      eng_chars_to_padding_mask = np.arange(eng_sentence_length + 1, max_token_length)\n",
        "      sp_chars_to_padding_mask = np.arange(sp_sentence_length + 1, max_token_length)\n",
        "      encoder_padding_mask[idx, :, eng_chars_to_padding_mask] = True\n",
        "      encoder_padding_mask[idx, eng_chars_to_padding_mask, :] = True\n",
        "      decoder_padding_mask_self_attention[idx, :, sp_chars_to_padding_mask] = True\n",
        "      decoder_padding_mask_self_attention[idx, sp_chars_to_padding_mask, :] = True\n",
        "      decoder_padding_mask_cross_attention[idx, :, eng_chars_to_padding_mask] = True\n",
        "      decoder_padding_mask_cross_attention[idx, sp_chars_to_padding_mask, :] = True\n",
        "\n",
        "    encoder_self_attention_mask = torch.where(encoder_padding_mask, NEG_INFTY, 0)\n",
        "    decoder_self_attention_mask =  torch.where(look_ahead_mask + decoder_padding_mask_self_attention, NEG_INFTY, 0)\n",
        "    decoder_cross_attention_mask = torch.where(decoder_padding_mask_cross_attention, NEG_INFTY, 0)\n",
        "    return encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8kyA6OsJWKA"
      },
      "source": [
        "Explaination 1 :\n",
        "\n",
        "encoder_self_attention_mask and decoder_cross_attention_mask are used so that transformer do not pay attention to the padding tokens (which is done by putting zeros (till the length of the sentence + 1) and remaning part in sequence is covered by -infinity)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        example:\n",
        "        max_sequence_length  = 8,   \n",
        "                  sentence =  'good'(len = 4),num_sentence = 1\n",
        "                  zero values should be till index len(sentence) + 1\n",
        "\n",
        "\n",
        "        mask = [[[0 0 0 0 0 -inf -inf -inf],\n",
        "                 [0 0 0 0 0 -inf -inf -inf],\n",
        "                 [0 0 0 0 0 -inf -inf -inf],\n",
        "                 [0 0 0 0 0 -inf -inf -inf],\n",
        "                 [0 0 0 0 0 -inf -inf -inf],\n",
        "                 [0 0 0 0 0 -inf -inf -inf],\n",
        "                 [0 0 0 0 0 -inf -inf -inf],\n",
        "                 [0 0 0 0 0 -inf -inf -inf]]]\n",
        "                shape (num_sentence,max_sequence_length,max_sequence_length)\n",
        "        \n",
        "        attention_weights = [[[0.00426842, 0.00752416, 0.00349225, 0.00898395, 0.00556792, -0.00568639, -0.00325177, 0.00724002],\n",
        "                             [0.00921758, 0.00367077, 0.00327958, 0.00263405, 0.00764168, -0.00392446, -0.00628386, 0.00685052],\n",
        "                             [0.00997227, 0.00228168, 0.00833329, 0.00146394, 0.00922879, -0.00393896, -0.00372312, 0.00919514],\n",
        "                             [0.00588389, 0.00815015, 0.00625498, 0.00393098, 0.0071409, -0.00682445, -0.00449244, 0.00170309],\n",
        "                             [0.00125849, 0.00692958, 0.00917532, 0.00639848, 0.00209307, -0.00023777, -0.00540265, 0.00118428],\n",
        "                             [0.00485591, 0.00195363, 0.00936389, 0.00918742, 0.00358588, -0.00993245, -0.00042846, 0.00660049],\n",
        "                             [0.00730663, 0.00275739, 0.00828811, 0.00286777, 0.00250849, -0.00248524, -0.00326519, 0.00197197],\n",
        "                             [0.00901291, 0.00702945, 0.00767226, 0.00873171, 0.0090118, -0.00064111, -0.00999714, 0.00365651]]]\n",
        "                             shape (batch_size = 1,max_sequence_length,max_sequence_length)\n",
        "\n",
        "\n",
        "        result =  (mask + attention_weights)\n",
        "                         [[[0.00426842, 0.00752416, 0.00349225, 0.00898395, 0.00556792, -inf, -inf, -inf],\n",
        "                         [0.00921758, 0.00367077, 0.00327958, 0.00263405, 0.00764168, -inf, -inf, -inf],\n",
        "                         [0.00997227, 0.00228168, 0.00833329, 0.00146394, 0.00922879, -inf, -inf, -inf],\n",
        "                         [0.00588389, 0.00815015, 0.00625498, 0.00393098, 0.0071409, -inf, -inf,-inf],\n",
        "                         [0.00125849, 0.00692958, 0.00917532, 0.00639848, 0.00209307, -inf, -inf, -inf],\n",
        "                         [0.00485591, 0.00195363, 0.00936389, 0.00918742, 0.00358588, -inf, -inf,-inf],\n",
        "                         [0.00730663, 0.00275739, 0.00828811, 0.00286777, 0.00250849, -inf, -inf, -inf],\n",
        "                         [0.00901291, 0.00702945, 0.00767226, 0.00873171, 0.0090118, -inf, -inf, -inf]]]\n",
        "            \n",
        "\n",
        "\n",
        "        softmax(result,dim = -1)\n",
        "                  [[[0.1997, 0.2003, 0.1995, 0.2006, 0.1999, 0.0000, 0.0000, 0.0000],\n",
        "                    [0.2008, 0.1997, 0.1996, 0.1995, 0.2005, 0.0000, 0.0000, 0.0000],\n",
        "                    [0.2007, 0.1992, 0.2004, 0.1990, 0.2006, 0.0000, 0.0000, 0.0000],\n",
        "                    [0.1999, 0.2004, 0.2000, 0.1995, 0.2002, 0.0000, 0.0000, 0.0000],\n",
        "                    [0.1992, 0.2004, 0.2008, 0.2002, 0.1994, 0.0000, 0.0000, 0.0000],\n",
        "                    [0.1998, 0.1992, 0.2007, 0.2007, 0.1996, 0.0000, 0.0000, 0.0000],\n",
        "                    [0.2005, 0.1996, 0.2007, 0.1996, 0.1996, 0.0000, 0.0000, 0.0000],\n",
        "                    [0.2001, 0.1997, 0.1999, 0.2001, 0.2001, 0.0000, 0.0000, 0.0000]]]\n",
        "\n",
        "                         \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Explaination 2 :\n",
        "\n",
        "decoder_self_attention_mask is mainly used by the decoder's first sublayer known as masked_self_attention which is used so that while producing target token ,decoder should not able to see(or attend)  the future token or words\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      example:\n",
        "        max_sequence_length  = 8,   \n",
        "                  sentence =  'good'(len = 4),num_sentence = 1\n",
        "                  zero values should be till index len(sentence) + 1\n",
        "\n",
        "\n",
        "        mask = [[[0 -inf -inf -inf -inf -inf -inf -inf],\n",
        "                 [0  0  -inf  -inf -inf -inf -inf -inf],\n",
        "                 [0  0   0   -inf  -inf -inf -inf -inf],\n",
        "                 [0  0   0   0  -inf -inf -inf -inf],\n",
        "                 [0  0   0   0   0 -inf -inf -inf],\n",
        "                 [-inf -inf -inf -inf -inf  -inf -inf -inf],\n",
        "                 [-inf -inf -inf -inf -inf -inf -inf -inf],\n",
        "                 [-inf -inf -inf -inf -inf -inf -inf -inf ]]\n",
        "                shape (num_sentence,max_sequence_length,max_sequence_length)\n",
        "\n",
        "\n",
        "                Last  few vectors are filled  with infinity values\n",
        "                because zeros have filled the total index which they can fill\n",
        "                that is (len(good) + 1) and after that all is padding token where we do not need to pay attention that's why after paying attention to the last token of the sequence ,the next rows are filled with -infinity\n",
        "\n",
        "                & Similar above steps..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jn6vcch0F9gu",
        "outputId": "d6cad8bd-095c-4c42-f22d-8237ca681b7e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 7, 7])"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eng_batch = (\"happy\",'good','joyful','sri ram jai ram')\n",
        "sp_batch = (\"happy\",'good','joyful','hbhwsd')\n",
        "\n",
        "encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(eng_batch,sp_batch)\n",
        "encoder_self_attention_mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6mxDm4iGzW5",
        "outputId": "2e6bffee-633e-4a7d-da63-3b3a60fc13fe"
      },
      "outputs": [],
      "source": [
        "encoder_self_attention_mask[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCZNSw677ERg",
        "outputId": "646f4e1f-a8f8-4d25-f672-96a5528f19f2"
      },
      "outputs": [],
      "source": [
        "decoder_self_attention_mask[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VD8i3sis7Nck",
        "outputId": "fa2b6f91-56de-49c0-d93d-c5db9357eea4"
      },
      "outputs": [],
      "source": [
        "decoder_cross_attention_mask[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HHcp31W_wNp",
        "outputId": "59818b3d-fde3-41d3-9357-1bea29c68694"
      },
      "outputs": [],
      "source": [
        "decoder_self_attention_mask[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WH24LbAc0_Dl",
        "outputId": "02a04128-3eb7-406a-f2cf-13e87a698806"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7326"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(en_index_to_lng)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwdwXFz4CYFZ",
        "outputId": "197008ea-6940-4d3c-c2fd-0c0b9e16b142"
      },
      "outputs": [],
      "source": [
        "transformer.train() # used to set the model in training mode\n",
        "# transformer.to(device)\n",
        "total_loss = 0\n",
        "num_epochs = 7\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  # print(f\"epoch {epoch}\")\n",
        "  iterator = iter(train_loader)\n",
        "  '''\n",
        "  looping on iterator we will get the batch of input_sequence and target_sequence\n",
        "  [(batch_of_input_sequence),(batch_of_target_sequence)]\n",
        "  '''\n",
        "\n",
        "  for batch_num,batch  in enumerate(iterator):\n",
        "    '''\n",
        "    batch_num{int}: current batch number\n",
        "    batch{list{tuple}}: batch of input_sequence and target_sequence\n",
        "    '''\n",
        "    transformer.train()\n",
        "    eng_batch,sp_batch = batch\n",
        "    # creating the mask for the eng_batch and sp_batch\n",
        "    encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(eng_batch, sp_batch)\n",
        "    '''\n",
        "    ***important***\n",
        "    {the gradients of each mini-batch should be computed independantly}\n",
        "\n",
        "    .zero-grad():set gradient to zero at the start of training of each mini-batch\n",
        "                 so that while backpropogation the gradient is not accumulated:\n",
        "                              (means gradient of these mini batch would not effect the gradient of other mini-batch)\n",
        "\n",
        "    '''\n",
        "    optim.zero_grad()\n",
        "\n",
        "    sp_predictions = transformer(eng_batch,\n",
        "                                 sp_batch,\n",
        "                                 encoder_self_attention_mask,\n",
        "                                 decoder_self_attention_mask,\n",
        "                                 decoder_cross_attention_mask,\n",
        "                                 enc_start_token = False,\n",
        "                                 enc_end_token = False,\n",
        "                                 dec_start_token = True,\n",
        "                                 dec_end_token = True)\n",
        "    '''\n",
        "    labels{tensor}:\n",
        "          converting spanish sentences into their index values based on spanish_to_index\n",
        "    '''\n",
        "    # print(sp_batch)\n",
        "    labels = transformer.decoder.sentence_embedding.batch_tokenize(sp_batch, start_token=False, end_token=True)  # shspe (batch_size,max_sequence_length) {max_sequence_length  = num_queries}\n",
        "    # print(labels.shape)\n",
        "    '''\n",
        "    loss = criterian(....):\n",
        "        represent loss of each mini-batch\n",
        "        by computimg loss over all characters(or tokens) in a batch of sentences\n",
        "        loss[:max_sequnce_length] = loss of 1st sentence of batch\n",
        "    '''\n",
        "    loss = criterian(\n",
        "        sp_predictions.view(-1,sp_vocab_size), # shape (batch_size * num_queries,sp_vocab_size)\n",
        "        labels.view(-1) # shape (bach_size * num_queries.)\n",
        "    ) # shape (batch_size * num_queries)\n",
        "    # print(loss[:50])\n",
        "\n",
        "    '''\n",
        "    valid_indices:\n",
        "                setting true value where labels are not padding token\n",
        "                and false value where labels are padding token\n",
        "    '''\n",
        "    valid_indicies = torch.where(labels.view(-1) == sp_lng_to_index[PADDING_TOKEN], False, True) # shape (batch_size * num_queries)\n",
        "    # print(valid_indicies[:50])\n",
        "    # print(loss.sum())\n",
        "    # print(valid_indicies.sum())\n",
        "    '''\n",
        "    loss = loss.sum()/valid...:\n",
        "                represent a loss value(single number), where the loss of all padding tokens are ignored\n",
        "\n",
        "    '''\n",
        "    loss = loss.sum() / valid_indicies.sum()\n",
        "    # print(loss)\n",
        "\n",
        "    '''\n",
        "    loss.backward():\n",
        "                compute gradient or derivative using the loss function and model's parameters\n",
        "                *** Theory ***\n",
        "                          L = loss function\n",
        "                          w = model's weight or pparameter\n",
        "                          dL/dw = gradient of loss function wrt weight w\n",
        "                          dL/dw:\n",
        "                                represent how the loss function is changing if we change the model's parameter w(increase or decrease w value)\n",
        "\n",
        "                                computed using chain rule of calculus\n",
        "\n",
        "    optim.step():\n",
        "              updating the model parameter with above computed gradient or derivative\n",
        "              using a specific optmizer equation{Adam,Gradient descent,SGD,...}\n",
        "\n",
        "    .item():\n",
        "            return the value of torch tensor which containe only one value\n",
        "\n",
        "    '''\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "\n",
        "    if (batch_num % 100 == 0):\n",
        "      print(f\"epoch {epoch} batch {batch_num} loss {loss.item()}\")\n",
        "      print(f\"English: {eng_batch[0]}\")\n",
        "      print(f\"Spanish Translation: {sp_batch[0]}\")\n",
        "      '''\n",
        "      sp_predictions[0]:\n",
        "                  represent the output of transformer for a first trainable example in a batch\n",
        "      sp_sentence_predicted:\n",
        "                  represent the predicted index of spanish sentence for a first trainable example in a batch\n",
        "      '''\n",
        "\n",
        "      sp_sentence_predicted = torch.argmax(sp_predictions[0], # shape (num_queries,sp_vocab_size)\n",
        "                                           axis=1)  # shape (num_queries,)\n",
        "      # print(sp_sentence_predicted)\n",
        "      predicted_sentence = \"\"\n",
        "      for idx in sp_sentence_predicted:\n",
        "        # print(idx.item())\n",
        "        if idx == sp_lng_to_index[END_TOKEN]:\n",
        "          break\n",
        "        predicted_sentence += sp_index_to_lng[idx.item()]\n",
        "      print(f'Spaniish Prediction: {predicted_sentence}')\n",
        "\n",
        "\n",
        "      transformer.eval()  # used to set the model in evaluation mode\n",
        "      '''\n",
        "      ***Important***\n",
        "         a =  ('dksdkk')\n",
        "         type(a),len(a)\n",
        "         --> str, 6\n",
        "         a =  ('dksdkk',)\n",
        "         type(a),len(a)\n",
        "         ---> tuple, 1\n",
        "      '''\n",
        "      sp_sentence = (\"\",)\n",
        "      eng_sentence  = [\"everyone is happy.\"]\n",
        "      for i,sentence in enumerate(eng_sentence):\n",
        "        eng_sentence[i] = text_preprocessing(sentence)\n",
        "      eng_sentence = tuple(eng_sentence)\n",
        "      for word_counter in range(max_token_length):\n",
        "          encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask= create_masks(eng_sentence, sp_sentence)\n",
        "          predictions = transformer(eng_sentence,\n",
        "                                          sp_sentence,\n",
        "                                          encoder_self_attention_mask,\n",
        "                                          decoder_self_attention_mask,\n",
        "                                          decoder_cross_attention_mask,\n",
        "                                          enc_start_token=False,\n",
        "                                          enc_end_token=False,\n",
        "                                          dec_start_token=True,\n",
        "                                          dec_end_token=False)\n",
        "          # print(predictions.shape)\n",
        "          '''\n",
        "          next_token_:\n",
        "                  contains raw prediction values for each token in vocabulary\n",
        "                  example:\n",
        "                        \"<start>i am happy<end>\"\n",
        "                        input token <start> feeded to transformer decoder\n",
        "                        then output of transformer represent raw prediction values,\n",
        "                        predicting what could be the next word (after <start> token) form the vocabulary\n",
        "                        shape (sp_vocab_size,)\n",
        "\n",
        "                                  I    <end>   a      m     h\n",
        "                        output = [21.5 , 7.9  , 0.1 , -20 , -2.4 ,...] {random_values}\n",
        "                        highest prediction value is the next word\n",
        "\n",
        "                        {real raw values range depnds on type activation function that is used}\n",
        "\n",
        "\n",
        "          '''\n",
        "          next_token_raw_distribution = predictions[0][word_counter] # shape (sp_vocab_size,)\n",
        "          # print(next_token_raw_distribution)\n",
        "          next_token_index = torch.argmax(next_token_raw_distribution).item()\n",
        "          # print(next_token_index)\n",
        "          next_token = sp_index_to_lng[next_token_index]\n",
        "          # print(next_token)\n",
        "          sp_sentence = (sp_sentence[0] + next_token,)\n",
        "          if (next_token == END_TOKEN):\n",
        "            break\n",
        "\n",
        "      print(f'Evaluation translation of {eng_sentence[0]} : {sp_sentence[0]}')\n",
        "      print(f'-----------------------------------------------------------')\n",
        "\n",
        "\n",
        "    # break\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8Yo_Zga6VSO"
      },
      "outputs": [],
      "source": [
        "len(sp_index_to_lng),len(sp_lng_to_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68Rc1STz5ggI"
      },
      "outputs": [],
      "source": [
        "sp_index_to_lng[3800]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaGJriaTNIRo"
      },
      "outputs": [],
      "source": [
        "transformer.eval()\n",
        "def translate(input_sentence):\n",
        "  '''\n",
        "      ***Important***\n",
        "         a =  ('dksdkk')\n",
        "         type(a),len(a)\n",
        "         --> str, 6\n",
        "         a =  ('dksdkk',)\n",
        "         type(a),len(a)\n",
        "         ---> tuple, 1\n",
        "      '''\n",
        "  sp_sentence = (\"\",)\n",
        "  eng_sentence  = [input_sentence]\n",
        "  for i,sentence in enumerate(eng_sentence):\n",
        "      eng_sentence[i] = text_preprocessing(sentence)\n",
        "  eng_sentence = tuple(eng_sentence)\n",
        "  for word_counter in range(max_token_length):\n",
        "      encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask= create_masks(eng_sentence, sp_sentence)\n",
        "      predictions = transformer(eng_sentence,\n",
        "                                          sp_sentence,\n",
        "                                          encoder_self_attention_mask,\n",
        "                                          decoder_self_attention_mask,\n",
        "                                          decoder_cross_attention_mask,\n",
        "                                          enc_start_token=False,\n",
        "                                          enc_end_token=False,\n",
        "                                          dec_start_token=True,\n",
        "                                          dec_end_token=False)\n",
        "      # print(predictions.shape)\n",
        "      '''\n",
        "      next_token_:\n",
        "              contains raw prediction values for each token in vocabulary\n",
        "              example:\n",
        "                    \"<start>i am happy<end>\"\n",
        "                    input token <start> feeded to transformer decoder\n",
        "                    then output of transformer represent raw prediction values,\n",
        "                    predicting what could be the next word (after <start> token) form the vocabulary\n",
        "                    shape (sp_vocab_size,)\n",
        "\n",
        "                              I    <end>   a      m     h\n",
        "                    output = [21.5 , 7.9  , 0.1 , -20 , -2.4 ,...] {random_values}\n",
        "                    highest prediction value is the next word\n",
        "\n",
        "                    {real raw values range depnds on type activation function that is used}\n",
        "\n",
        "\n",
        "      '''\n",
        "      next_token_raw_distribution = predictions[0][word_counter] # shape (sp_vocab_size,)\n",
        "      # print(next_token_raw_distribution)\n",
        "      next_token_index = torch.argmax(next_token_raw_distribution).item()\n",
        "      # print(next_token_index)\n",
        "      next_token = sp_index_to_lng[next_token_index]\n",
        "      # print(next_token)\n",
        "      sp_sentence = (sp_sentence[0] + next_token,)\n",
        "      if (next_token == END_TOKEN):\n",
        "        break\n",
        "\n",
        "  print(f'Evaluation translation of  {eng_sentence[0]} : {sp_sentence[0]}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6Gm1FFlKLC6"
      },
      "outputs": [],
      "source": [
        "translate(\"i am happy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eR4mHUlXLf2s"
      },
      "outputs": [],
      "source": [
        "translate('i am sad')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
