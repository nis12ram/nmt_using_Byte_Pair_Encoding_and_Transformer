{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qe4n0y_He3Km"
      },
      "outputs": [],
      "source": [
        "from byte_pair_level_transformer import Transformer,SentenceEmbedding\n",
        "import torch\n",
        "import numpy as np\n",
        "import re\n",
        "import collections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1pBj3ma7hVc4"
      },
      "outputs": [],
      "source": [
        "file_path = r'D:\\\\test_locally_notebooks\\\\test_pytorch\\\\solving_mak_in_BPE\\\\spa.txt'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ge2rvFA0yZuR"
      },
      "outputs": [],
      "source": [
        "def text_preprocessing(text):\n",
        "  # text = re.sub(r\"([?.!,¿])\", r\" \\1 \", text)\n",
        "  text = re.sub(r'[\" \"]+', \" \", text)\n",
        "  # text = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", text)\n",
        "  text = re.sub(r\"[^a-zA-Z]+\", \" \", text)\n",
        "\n",
        "  text = text.strip().lower()\n",
        "\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ViD2B7hgyuKx"
      },
      "outputs": [],
      "source": [
        "num_data = 5000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8zWU4rSJik85"
      },
      "outputs": [],
      "source": [
        "with open(file_path,'r',encoding='utf-8') as f:\n",
        "  lines = f.readlines()\n",
        "english_sentence,spanish_sentence = [],[]\n",
        "for total_example,line in enumerate(lines):\n",
        "  if (total_example < num_data ):\n",
        "    line = line.lower()\n",
        "    data = line.replace(\"\\n\",\"\").split(\"\\t\")\n",
        "    data[0] = text_preprocessing(data[0])\n",
        "    data[1] = text_preprocessing(data[1])\n",
        "    english_sentence.append(data[0])\n",
        "    spanish_sentence.append(data[1])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "S6x1jFuOR7tn"
      },
      "outputs": [],
      "source": [
        "spanish_vocabulary = []\n",
        "english_vocabulary = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "t_3PTj5uLsiw"
      },
      "outputs": [],
      "source": [
        "for en,sp in zip(english_sentence,spanish_sentence):\n",
        "  en_tokens = en.split()\n",
        "  sp_tokens = sp.split()\n",
        "  for en_token,sp_token in zip(en_tokens,sp_tokens):\n",
        "    if en_token not in english_vocabulary:\n",
        "      english_vocabulary.append(en_token)\n",
        "    if sp_token not in spanish_vocabulary:\n",
        "      spanish_vocabulary.append(sp_token)\n",
        "  # print(english_vocabulary)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8Dv2I1FOKub",
        "outputId": "ca795012-744d-4673-ba8a-290af9ecc594"
      },
      "outputs": [],
      "source": [
        "len(english_vocabulary),len(spanish_vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "a8BnIXcSngHJ"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "en_word_freq_dict  = collections.defaultdict(int)\n",
        "\n",
        "for word in english_vocabulary:\n",
        "  en_word_freq_dict[\" \".join(word) + ' #'] += 1\n",
        "# en_word_freq_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "qr0fiCGbn7OQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "sp_word_freq_dict  = collections.defaultdict(int)\n",
        "\n",
        "for word in spanish_vocabulary:\n",
        "  sp_word_freq_dict[\" \".join(word) + ' #'] += 1\n",
        "# sp_word_freq_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "t8xezAN3nDI-"
      },
      "outputs": [],
      "source": [
        "def get_pairs(word_freq_dict):\n",
        "  '''\n",
        "  goal:\n",
        "      used to get the pairs dict:\n",
        "                key(tuple{str,str}): represent the byte pairs\n",
        "                values(int): represent the frequency of the byte pair\n",
        "      and return the pairs dict\n",
        "  '''\n",
        "  pairs = collections.defaultdict(int)\n",
        "  for word, freq in word_freq_dict.items():\n",
        "    chars = word.split()\n",
        "    for i in range(len(chars)-1):\n",
        "      pairs[chars[i],chars[i+1]] += freq\n",
        "  return pairs\n",
        "\n",
        "def merge_byte_pairs(best_pair, word_freq_dict):\n",
        "  '''\n",
        "  goal:\n",
        "      used to merge the byte pairs that has highest frequency\n",
        "      and return the merged dict{new word_freq_dict}\n",
        "  '''\n",
        "  # print(best_pair)\n",
        "  merged_dict = {}\n",
        "  bigram = re.escape(' '.join(best_pair))\n",
        "  # print(f'bigram {bigram}')\n",
        "  p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
        "  # print(f'p {p}')\n",
        "  for word in word_freq_dict:\n",
        "    # print(word)\n",
        "    w_out = p.sub(''.join(best_pair), word) # merging best byte pair.\n",
        "    # print(f'w_out {w_out}')\n",
        "    merged_dict[w_out] = word_freq_dict[word]\n",
        "  return merged_dict\n",
        "\n",
        "def get_subword_tokens(word_freq_dict):\n",
        "  char_freq_dict = collections.defaultdict(int)\n",
        "  for word,freq in word_freq_dict.items():\n",
        "    chars = word.split()\n",
        "    for char in chars:\n",
        "      char_freq_dict[char] += freq\n",
        "  return char_freq_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "iZa_uxwdojLB"
      },
      "outputs": [],
      "source": [
        "for i in range(1000):\n",
        "  # if (i == 0):\n",
        "    # print(f'{get_subword_tokens(en_word_freq_dict)}')\n",
        "    # print('')\n",
        "  pairs = get_pairs(en_word_freq_dict)\n",
        "  try:\n",
        "    best_pair = max(pairs,key = pairs.get)\n",
        "  except:\n",
        "    break\n",
        "  # print(f\"Iteration {i}: \")\n",
        "  en_word_freq_dict = merge_byte_pairs(best_pair,en_word_freq_dict)\n",
        "  en_subword_tokens = get_subword_tokens(en_word_freq_dict)\n",
        "  # print(en_subword_tokens)\n",
        "  # print(len(en_subword_tokens))\n",
        "  # print()\n",
        "  i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Zf1X-CK3o2iY"
      },
      "outputs": [],
      "source": [
        "\n",
        "for i in range(1000):\n",
        "  # if (i == 0):\n",
        "  #   print(f'{get_subword_tokens(sp_word_freq_dict)}')\n",
        "  #   print('')\n",
        "  pairs = get_pairs(sp_word_freq_dict)\n",
        "  try:\n",
        "    best_pair = max(pairs,key = pairs.get)\n",
        "  except:\n",
        "    break\n",
        "  # print(f\"Iteration {i}: \")\n",
        "  sp_word_freq_dict = merge_byte_pairs(best_pair,sp_word_freq_dict)\n",
        "  sp_subword_tokens = get_subword_tokens(sp_word_freq_dict)\n",
        "  # print(sp_subword_tokens)\n",
        "  # print(len(sp_subword_tokens))\n",
        "  # print()\n",
        "  i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "pnqg1ws4dQsK"
      },
      "outputs": [],
      "source": [
        "def measure_token_length(token):\n",
        "    if token[-4:] == '#':\n",
        "        return len(token[:-4]) + 1\n",
        "    else:\n",
        "        return len(token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPn88g2Nkhg6"
      },
      "source": [
        "Byte pair vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "elC0oO-qdGSq"
      },
      "outputs": [],
      "source": [
        "en_sorted_tokens_tuple = sorted(en_subword_tokens.items(), key=lambda item: (measure_token_length(item[0]), item[1]), reverse=True)\n",
        "sp_sorted_tokens_tuple = sorted(sp_subword_tokens.items(), key=lambda item: (measure_token_length(item[0]), item[1]), reverse=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-XffxrmeeHr",
        "outputId": "5a2b64fd-703e-486c-c8f7-6d393614d62a"
      },
      "outputs": [],
      "source": [
        "en_sorted_tokens_tuple[0],en_sorted_tokens_tuple[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "0bp_g3MF5iL2"
      },
      "outputs": [],
      "source": [
        "en_vocab_tokenization = [token for (token, freq) in en_sorted_tokens_tuple]\n",
        "sp_vocab_tokenization = [token for (token, freq) in sp_sorted_tokens_tuple]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73vakBe85kjb",
        "outputId": "df2f7cab-cd5a-4d34-e9c1-32949e8f986d"
      },
      "outputs": [],
      "source": [
        "len(en_vocab_tokenization),len(sp_vocab_tokenization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJKXNramfnv3",
        "outputId": "4c08df31-1422-4e57-94ba-9de20d717f9f"
      },
      "outputs": [],
      "source": [
        "en_vocab_tokenization[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "1irLJNPW5m7a"
      },
      "outputs": [],
      "source": [
        "en_lng_to_index = {}\n",
        "en_index_to_lng = {}\n",
        "k = 4\n",
        "for i in en_vocab_tokenization:\n",
        "  en_lng_to_index[i] = k\n",
        "  en_index_to_lng[k] = i\n",
        "  k += 1\n",
        "\n",
        "\n",
        "sp_lng_to_index = {}\n",
        "sp_index_to_lng = {}\n",
        "k = 4\n",
        "for i in sp_vocab_tokenization:\n",
        "  sp_lng_to_index[i] = k\n",
        "  sp_index_to_lng[k] = i\n",
        "  k += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "-T2GFwgc2cuV"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "START_TOKEN = '<START>'\n",
        "PADDING_TOKEN = '<PAD>'\n",
        "END_TOKEN = '<END>'\n",
        "\n",
        "en_index_to_lng[0] = '<OOV>'\n",
        "sp_index_to_lng[0] = '<OOV>'\n",
        "en_index_to_lng[1] = '<START>'\n",
        "sp_index_to_lng[1] = '<START>'\n",
        "en_index_to_lng[2] = '<END>'\n",
        "sp_index_to_lng[2] = '<END>'\n",
        "en_index_to_lng[3] = '<PAD>'\n",
        "sp_index_to_lng[3] = '<PAD>'\n",
        "\n",
        "en_lng_to_index['<OOV>'] = 0\n",
        "sp_lng_to_index['<OOV>'] = 0\n",
        "en_lng_to_index['<START>'] = 1\n",
        "sp_lng_to_index['<START>'] = 1\n",
        "en_lng_to_index['<END>'] = 2\n",
        "sp_lng_to_index['<END>'] = 2\n",
        "en_lng_to_index['<PAD>'] = 3\n",
        "sp_lng_to_index['<PAD>'] = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhO0WBLX5pXF",
        "outputId": "b2f16798-1dc8-4e97-eacd-a07d36278818"
      },
      "outputs": [],
      "source": [
        "en_lng_to_index,sp_lng_to_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hNF0kdQ5zGW",
        "outputId": "95fe01d0-f98a-4156-bc66-512f9c02df2e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5000, 5000)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(english_sentence),len(spanish_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0gpYOyFkQFy",
        "outputId": "c6c78dd1-5336-45c7-a227-b89db042edb6"
      },
      "outputs": [],
      "source": [
        "english_sentence[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dv7VWb8GkWHh",
        "outputId": "ba8f6953-3a2a-46f6-89e8-9a342ff8b724"
      },
      "outputs": [],
      "source": [
        "spanish_sentence[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYljiEZkkiA0",
        "outputId": "9abc2458-63fc-45db-d38f-8a3c88629128"
      },
      "outputs": [],
      "source": [
        "max(len(x) for x in english_sentence),max(len(x) for x in spanish_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1g1mTQwkwt6",
        "outputId": "1de9af76-0405-4d82-bba0-2258e1a2c9dd"
      },
      "outputs": [],
      "source": [
        "# computing avg length\n",
        "print(sum(len(x) for x in english_sentence)/len(english_sentence))\n",
        "print(sum(len(x) for x in spanish_sentence)/len(spanish_sentence))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "vtca6awt9J_z"
      },
      "outputs": [],
      "source": [
        "d_model = 152\n",
        "batch_size = 16\n",
        "ffn_hidden = 152\n",
        "num_heads = 2\n",
        "drop_prob = 0.1\n",
        "num_stacked = 1\n",
        "max_token_length = 17\n",
        "sp_vocab_size = len(sp_lng_to_index)\n",
        "en_vocab_size = len(en_lng_to_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkbD6SsvsbYv"
      },
      "source": [
        "limit the length of the sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhCxoQqzq4o3",
        "outputId": "bd7a5a0f-f670-46f3-8f76-42d40853c56b"
      },
      "outputs": [],
      "source": [
        "max(len(x) for x in english_sentence),max(len(x) for x in spanish_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "O5am0HOPuWHz"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "  '''\n",
        "  overriding certain methods of the Dataset class\n",
        "  '''\n",
        "  def __init__(self,english_sentence,spanish_sentence):\n",
        "    self.english_sentence = english_sentence\n",
        "    self.spanish_sentence = spanish_sentence\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.english_sentence)\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    return self.english_sentence[index],self.spanish_sentence[index]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "Y0q7IGjZvVEH"
      },
      "outputs": [],
      "source": [
        "dataset = TextDataset(english_sentence,spanish_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eI5iOYbvX77",
        "outputId": "9aa06219-384e-4fc6-9b27-2456cf982bad"
      },
      "outputs": [],
      "source": [
        "len(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "KRLIKa_tv_CJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_loader = DataLoader(dataset,batch_size=batch_size)\n",
        "iterator = iter(train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBTgtR9cwUSA",
        "outputId": "e4d9af25-f902-4b07-cbdb-676766e1a554"
      },
      "outputs": [],
      "source": [
        "for batch_num,batch in enumerate(iterator):\n",
        "  print(batch)\n",
        "  # break\n",
        "  if (batch_num > 3):\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "eVHzNQWAA8g2"
      },
      "outputs": [],
      "source": [
        "transformer = Transformer(d_model,\n",
        "                          ffn_hidden,\n",
        "                          num_heads,\n",
        "                          drop_prob,\n",
        "                          num_stacked,\n",
        "                          max_token_length,\n",
        "                          sp_vocab_size,\n",
        "                          en_lng_to_index,\n",
        "                          sp_lng_to_index,\n",
        "                          START_TOKEN,\n",
        "                          END_TOKEN,\n",
        "                          PADDING_TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkQmg8k5DCXh",
        "outputId": "14ed3785-470a-45c6-f92d-9b44fe2bf251"
      },
      "outputs": [],
      "source": [
        "transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "meuR2adX-h8y"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "criterian = nn.CrossEntropyLoss(ignore_index=sp_lng_to_index[PADDING_TOKEN],\n",
        "                                reduction='none')\n",
        "\n",
        "# When computing the loss, we are ignoring cases when the label is the padding token\n",
        "for params in transformer.parameters():\n",
        "    if params.dim() > 1:\n",
        "        nn.init.xavier_uniform_(params)\n",
        "\n",
        "optim = torch.optim.Adam(transformer.parameters(), lr=1e-4)\n",
        "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "GTV2rZfRDk15"
      },
      "outputs": [],
      "source": [
        "NEG_INFTY = -1e9\n",
        "\n",
        "def create_masks(eng_base_mask, sp_base_mask):\n",
        "    num_sentences = eng_base_mask.shape[0] # {represent batch size}\n",
        "    look_ahead_mask = torch.full([max_token_length, max_token_length] , True)\n",
        "    look_ahead_mask = torch.triu(look_ahead_mask, diagonal=1)\n",
        "    encoder_padding_mask = torch.full([num_sentences, max_token_length, max_token_length] , False)\n",
        "    decoder_padding_mask_self_attention = torch.full([num_sentences, max_token_length, max_token_length] , False)\n",
        "    decoder_padding_mask_cross_attention = torch.full([num_sentences, max_token_length, max_token_length] , False)\n",
        "\n",
        "    for idx in range(num_sentences):\n",
        "      eng_sentence_length,sp_sentence_length = 0,0\n",
        "      for en_i,sp_i in zip(eng_base_mask[idx],sp_base_mask[idx]):\n",
        "        if en_i.item() != 3:\n",
        "          eng_sentence_length += 1\n",
        "        if sp_i.item() != 3:\n",
        "          sp_sentence_length += 1\n",
        "      eng_sentence_length, sp_sentence_length = len(eng_batch[idx]), len(sp_batch[idx])\n",
        "      eng_chars_to_padding_mask = np.arange(eng_sentence_length , max_token_length)\n",
        "      sp_chars_to_padding_mask = np.arange(sp_sentence_length + 1, max_token_length)\n",
        "      encoder_padding_mask[idx, :, eng_chars_to_padding_mask] = True\n",
        "      encoder_padding_mask[idx, eng_chars_to_padding_mask, :] = True\n",
        "      decoder_padding_mask_self_attention[idx, :, sp_chars_to_padding_mask] = True\n",
        "      decoder_padding_mask_self_attention[idx, sp_chars_to_padding_mask, :] = True\n",
        "      decoder_padding_mask_cross_attention[idx, :, eng_chars_to_padding_mask] = True\n",
        "      decoder_padding_mask_cross_attention[idx, sp_chars_to_padding_mask, :] = True\n",
        "\n",
        "    encoder_self_attention_mask = torch.where(encoder_padding_mask, NEG_INFTY, 0)\n",
        "    decoder_self_attention_mask =  torch.where(look_ahead_mask + decoder_padding_mask_self_attention, NEG_INFTY, 0)\n",
        "    decoder_cross_attention_mask = torch.where(decoder_padding_mask_cross_attention, NEG_INFTY, 0)\n",
        "    return encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "sentence_embedding = SentenceEmbedding(\n",
        "                          \n",
        "                          max_token_length,\n",
        "                          d_model,\n",
        "                          en_lng_to_index,\n",
        "                          START_TOKEN,\n",
        "                          END_TOKEN,\n",
        "                          PADDING_TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jn6vcch0F9gu",
        "outputId": "d6cad8bd-095c-4c42-f22d-8237ca681b7e"
      },
      "outputs": [],
      "source": [
        "eng_batch = (\"happy\",'good','joyful','sri ram jai ram')\n",
        "sp_batch = (\"happy\",'good','joyful','hbhwsd')\n",
        "\n",
        "encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(sentence_embedding.batch_tokenize(eng_batch,False,False),sentence_embedding.batch_tokenize(sp_batch,False,False))\n",
        "encoder_self_attention_mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6mxDm4iGzW5",
        "outputId": "2e6bffee-633e-4a7d-da63-3b3a60fc13fe"
      },
      "outputs": [],
      "source": [
        "encoder_self_attention_mask[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCZNSw677ERg",
        "outputId": "646f4e1f-a8f8-4d25-f672-96a5528f19f2"
      },
      "outputs": [],
      "source": [
        "decoder_self_attention_mask[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VD8i3sis7Nck",
        "outputId": "fa2b6f91-56de-49c0-d93d-c5db9357eea4"
      },
      "outputs": [],
      "source": [
        "decoder_cross_attention_mask[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HHcp31W_wNp",
        "outputId": "59818b3d-fde3-41d3-9357-1bea29c68694"
      },
      "outputs": [],
      "source": [
        "decoder_self_attention_mask[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WH24LbAc0_Dl",
        "outputId": "02a04128-3eb7-406a-f2cf-13e87a698806"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "910"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(en_index_to_lng)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwdwXFz4CYFZ",
        "outputId": "197008ea-6940-4d3c-c2fd-0c0b9e16b142"
      },
      "outputs": [],
      "source": [
        "transformer.train() # used to set the model in training mode\n",
        "# transformer.to(device)\n",
        "total_loss = 0\n",
        "num_epochs = 7\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  # print(f\"epoch {epoch}\")\n",
        "  iterator = iter(train_loader)\n",
        "  '''\n",
        "  looping on iterator we will get the batch of input_sequence and target_sequence\n",
        "  [(batch_of_input_sequence),(batch_of_target_sequence)]\n",
        "  '''\n",
        "\n",
        "  for batch_num,batch  in enumerate(iterator):\n",
        "    '''\n",
        "    batch_num{int}: current batch number\n",
        "    batch{list{tuple}}: batch of input_sequence and target_sequence\n",
        "    '''\n",
        "    transformer.train()\n",
        "    eng_batch,sp_batch = batch\n",
        "    # creating the mask for the eng_batch and sp_batch\n",
        "    encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(sentence_embedding.batch_tokenize(eng_batch,False,False),sentence_embedding.batch_tokenize(sp_batch,False,False))\n",
        "    '''\n",
        "    ***important***\n",
        "    {the gradients of each mini-batch should be computed independantly}\n",
        "\n",
        "    .zero-grad():set gradient to zero at the start of training of each mini-batch\n",
        "                 so that while backpropogation the gradient is not accumulated:\n",
        "                              (means gradient of these mini batch would not effect the gradient of other mini-batch)\n",
        "\n",
        "    '''\n",
        "    optim.zero_grad()\n",
        "\n",
        "    sp_predictions = transformer(eng_batch,\n",
        "                                 sp_batch,\n",
        "                                 encoder_self_attention_mask,\n",
        "                                 decoder_self_attention_mask,\n",
        "                                 decoder_cross_attention_mask,\n",
        "                                 enc_start_token = False,\n",
        "                                 enc_end_token = False,\n",
        "                                 dec_start_token = True,\n",
        "                                 dec_end_token = True)\n",
        "    '''\n",
        "    labels{tensor}:\n",
        "          converting spanish sentences into their index values based on spanish_to_index\n",
        "    '''\n",
        "    # print(sp_batch)\n",
        "    labels = transformer.decoder.sentence_embedding.batch_tokenize(sp_batch, start_token=False, end_token=True)  # shspe (batch_size,max_sequence_length) {max_sequence_length  = num_queries}\n",
        "    # print(labels.shape)\n",
        "    '''\n",
        "    loss = criterian(....):\n",
        "        represent loss of each mini-batch\n",
        "        by computimg loss over all characters(or tokens) in a batch of sentences\n",
        "        loss[:max_sequnce_length] = loss of 1st sentence of batch\n",
        "    '''\n",
        "    loss = criterian(\n",
        "        sp_predictions.view(-1,sp_vocab_size), # shape (batch_size * num_queries,sp_vocab_size)\n",
        "        labels.view(-1) # shape (bach_size * num_queries.)\n",
        "    ) # shape (batch_size * num_queries)\n",
        "    # print(loss[:50])\n",
        "\n",
        "    '''\n",
        "    valid_indices:\n",
        "                setting true value where labels are not padding token\n",
        "                and false value where labels are padding token\n",
        "    '''\n",
        "    valid_indicies = torch.where(labels.view(-1) == sp_lng_to_index[PADDING_TOKEN], False, True) # shape (batch_size * num_queries)\n",
        "    # print(valid_indicies[:50])\n",
        "    # print(loss.sum())\n",
        "    # print(valid_indicies.sum())\n",
        "    '''\n",
        "    loss = loss.sum()/valid...:\n",
        "                represent a loss value(single number), where the loss of all padding tokens are ignored\n",
        "\n",
        "    '''\n",
        "    loss = loss.sum() / valid_indicies.sum()\n",
        "    # print(loss)\n",
        "\n",
        "    '''\n",
        "    loss.backward():\n",
        "                compute gradient or derivative using the loss function and model's parameters\n",
        "                *** Theory ***\n",
        "                          L = loss function\n",
        "                          w = model's weight or pparameter\n",
        "                          dL/dw = gradient of loss function wrt weight w\n",
        "                          dL/dw:\n",
        "                                represent how the loss function is changing if we change the model's parameter w(increase or decrease w value)\n",
        "\n",
        "                                computed using chain rule of calculus\n",
        "\n",
        "    optim.step():\n",
        "              updating the model parameter with above computed gradient or derivative\n",
        "              using a specific optmizer equation{Adam,Gradient descent,SGD,...}\n",
        "\n",
        "    .item():\n",
        "            return the value of torch tensor which containe only one value\n",
        "\n",
        "    '''\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "\n",
        "    if (batch_num % 100 == 0):\n",
        "      print(f\"epoch {epoch} batch {batch_num} loss {loss.item()}\")\n",
        "      print(f\"English: {eng_batch[0]}\")\n",
        "      print(f\"Spanish Translation: {sp_batch[0]}\")\n",
        "      '''\n",
        "      sp_predictions[0]:\n",
        "                  represent the output of transformer for a first trainable example in a batch\n",
        "      sp_sentence_predicted:\n",
        "                  represent the predicted index of spanish sentence for a first trainable example in a batch\n",
        "      '''\n",
        "\n",
        "      sp_sentence_predicted = torch.argmax(sp_predictions[0], # shape (num_queries,sp_vocab_size)\n",
        "                                           axis=1)  # shape (num_queries,)\n",
        "      # print(sp_sentence_predicted)\n",
        "      predicted_sentence = \"\"\n",
        "      for idx in sp_sentence_predicted:\n",
        "        # print(idx.item())\n",
        "        if idx == sp_lng_to_index[END_TOKEN]:\n",
        "          break\n",
        "        predicted_sentence += sp_index_to_lng[idx.item()]\n",
        "      print(f'Spaniish Prediction: {predicted_sentence}')\n",
        "\n",
        "\n",
        "      transformer.eval()  # used to set the model in evaluation mode\n",
        "      '''\n",
        "      ***Important***\n",
        "         a =  ('dksdkk')\n",
        "         type(a),len(a)\n",
        "         --> str, 6\n",
        "         a =  ('dksdkk',)\n",
        "         type(a),len(a)\n",
        "         ---> tuple, 1\n",
        "      '''\n",
        "      sp_sentence = (\"\",)\n",
        "      eng_sentence  = [\"everyone is happy.\"]\n",
        "      for i,sentence in enumerate(eng_sentence):\n",
        "        eng_sentence[i] = text_preprocessing(sentence)\n",
        "      eng_sentence = tuple(eng_sentence)\n",
        "      for word_counter in range(max_token_length):\n",
        "          encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(sentence_embedding.batch_tokenize(eng_sentence,False,False),sentence_embedding.batch_tokenize(sp_sentence,False,False))\n",
        "          predictions = transformer(eng_sentence,\n",
        "                                          sp_sentence,\n",
        "                                          encoder_self_attention_mask,\n",
        "                                          decoder_self_attention_mask,\n",
        "                                          decoder_cross_attention_mask,\n",
        "                                          enc_start_token=False,\n",
        "                                          enc_end_token=False,\n",
        "                                          dec_start_token=True,\n",
        "                                          dec_end_token=False)\n",
        "          # print(predictions.shape)\n",
        "          '''\n",
        "          next_token_:\n",
        "                  contains raw prediction values for each token in vocabulary\n",
        "                  example:\n",
        "                        \"<start>i am happy<end>\"\n",
        "                        input token <start> feeded to transformer decoder\n",
        "                        then output of transformer represent raw prediction values,\n",
        "                        predicting what could be the next word (after <start> token) form the vocabulary\n",
        "                        shape (sp_vocab_size,)\n",
        "\n",
        "                                  I    <end>   a      m     h\n",
        "                        output = [21.5 , 7.9  , 0.1 , -20 , -2.4 ,...] {random_values}\n",
        "                        highest prediction value is the next word\n",
        "\n",
        "                        {real raw values range depnds on type activation function that is used}\n",
        "\n",
        "\n",
        "          '''\n",
        "          next_token_raw_distribution = predictions[0][word_counter] # shape (sp_vocab_size,)\n",
        "          # print(next_token_raw_distribution)\n",
        "          next_token_index = torch.argmax(next_token_raw_distribution).item()\n",
        "          # print(next_token_index)\n",
        "          next_token = sp_index_to_lng[next_token_index]\n",
        "          # print(next_token)\n",
        "          sp_sentence = (sp_sentence[0] + next_token,)\n",
        "          if (next_token == END_TOKEN):\n",
        "            break\n",
        "\n",
        "      print(f'Evaluation translation of {eng_sentence[0]} : {sp_sentence[0]}')\n",
        "      print(f'-----------------------------------------------------------')\n",
        "\n",
        "\n",
        "    # break\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8Yo_Zga6VSO"
      },
      "outputs": [],
      "source": [
        "len(sp_index_to_lng),len(sp_lng_to_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68Rc1STz5ggI"
      },
      "outputs": [],
      "source": [
        "sp_index_to_lng[3800]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaGJriaTNIRo"
      },
      "outputs": [],
      "source": [
        "transformer.eval()\n",
        "def translate(input_sentence):\n",
        "  '''\n",
        "      ***Important***\n",
        "         a =  ('dksdkk')\n",
        "         type(a),len(a)\n",
        "         --> str, 6\n",
        "         a =  ('dksdkk',)\n",
        "         type(a),len(a)\n",
        "         ---> tuple, 1\n",
        "      '''\n",
        "  sp_sentence = (\"\",)\n",
        "  eng_sentence  = [input_sentence]\n",
        "  for i,sentence in enumerate(eng_sentence):\n",
        "      eng_sentence[i] = text_preprocessing(sentence)\n",
        "  eng_sentence = tuple(eng_sentence)\n",
        "  for word_counter in range(max_token_length):\n",
        "      encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(sentence_embedding.batch_tokenize(eng_sentence,False,False),sentence_embedding.batch_tokenize(sp_sentence,False,False))\n",
        "      predictions = transformer(eng_sentence,\n",
        "                                          sp_sentence,\n",
        "                                          encoder_self_attention_mask,\n",
        "                                          decoder_self_attention_mask,\n",
        "                                          decoder_cross_attention_mask,\n",
        "                                          enc_start_token=False,\n",
        "                                          enc_end_token=False,\n",
        "                                          dec_start_token=True,\n",
        "                                          dec_end_token=False)\n",
        "      # print(predictions.shape)\n",
        "      '''\n",
        "      next_token_:\n",
        "              contains raw prediction values for each token in vocabulary\n",
        "              example:\n",
        "                    \"<start>i am happy<end>\"\n",
        "                    input token <start> feeded to transformer decoder\n",
        "                    then output of transformer represent raw prediction values,\n",
        "                    predicting what could be the next word (after <start> token) form the vocabulary\n",
        "                    shape (sp_vocab_size,)\n",
        "\n",
        "                              I    <end>   a      m     h\n",
        "                    output = [21.5 , 7.9  , 0.1 , -20 , -2.4 ,...] {random_values}\n",
        "                    highest prediction value is the next word\n",
        "\n",
        "                    {real raw values range depnds on type activation function that is used}\n",
        "\n",
        "\n",
        "      '''\n",
        "      next_token_raw_distribution = predictions[0][word_counter] # shape (sp_vocab_size,)\n",
        "      # print(next_token_raw_distribution)\n",
        "      next_token_index = torch.argmax(next_token_raw_distribution).item()\n",
        "      # print(next_token_index)\n",
        "      next_token = sp_index_to_lng[next_token_index]\n",
        "      # print(next_token)\n",
        "      sp_sentence = (sp_sentence[0] + next_token,)\n",
        "      if (next_token == END_TOKEN):\n",
        "        break\n",
        "\n",
        "  print(f'Evaluation translation of  {eng_sentence[0]} : {sp_sentence[0]}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6Gm1FFlKLC6"
      },
      "outputs": [],
      "source": [
        "translate(\"i am happy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eR4mHUlXLf2s"
      },
      "outputs": [],
      "source": [
        "translate('i am sad')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
